{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04e5dd2-25e2-41f6-8aa2-d5d8330673a3",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404cdd35-ddcc-483a-ae86-023ca7cf8f7b",
   "metadata": {},
   "source": [
    "# **Pre-training LLMs with Hugging Face**\n",
    "\n",
    "Estimated time: **45** minutes\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This project aims to introduce you to the process of pretraining large language models (LLMs) using the popular Hugging Face library. Hugging Face is a leading open-source platform for natural language processing that provides a wide range of pretrained models and tools for fine-tuning and deploying these models.\n",
    "\n",
    "You will learn how to load pre-trained models from Hugging Face and make inferences using the Pipeline module. Additionally, you will learn how to further train pre-trained LLMs on your own data (self-supervised fine-tuning). By the end of this lab, you will have a solid understanding of how to pretrain LLMs and store them to later fine-tune for your specific use cases. This will empower you to create powerful and customized natural language processing solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21cc35-e624-41f7-af2f-0b501808ef63",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Pretraining-and-self-supervised-fine-tuning\">Pretraining and self-supervised fine-tuning</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Importing-required-datasets\">Importing required datasets</a></li>\n",
    "            <li><a href=\"#Loading-the-saved-model\">Loading the saved model</a></li>\n",
    "            <li><a href=\"#Inferencing-a-pretrained-BERT-model\">Inferencing a pretrained BERT model</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Exercise\">Exercise</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89a0ac-45a2-4119-90e1-c3af35235136",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b59b7-2965-4c82-961a-a7676a218102",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "\n",
    " - Load pretrained LLMs from Hugging Face and make inferences using the pipeline module\n",
    " - Train pretrained LLMs on your data \n",
    " - Store LLMs to fine-tune them for specific use cases\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0534c8-54bf-4cd2-a023-ed0e2b51ca64",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39d65a-fdfd-475f-a25b-7fb4f5f34895",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91ac30-9e7a-40d9-8a3e-f7cfa43b9495",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `%pip` in the code cell below:\n",
    "\n",
    "_PS: To run lab this in your own environment, please note that the versions of libraries may differ due to dependencies._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db053270-b628-48be-adfa-ad00b4296913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  d:\\Course\\IBMGenAI\\venv\\Scripts\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  d:\\Course\\IBMGenAI\\venv\\Scripts\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  d:\\Course\\IBMGenAI\\venv\\Scripts\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
      "  d:\\Course\\IBMGenAI\\venv\\Scripts\\python.exe -m pip install [options] [-e] <local project path> ...\n",
      "  d:\\Course\\IBMGenAI\\venv\\Scripts\\python.exe -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pmdarima\n",
      "  Downloading pmdarima-2.0.4-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting joblib>=0.11 (from pmdarima)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting Cython!=0.29.18,!=0.29.31,>=0.29 (from pmdarima)\n",
      "  Downloading Cython-3.0.11-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting numpy>=1.21.2 (from pmdarima)\n",
      "  Downloading numpy-2.2.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pandas>=0.19 (from pmdarima)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting scikit-learn>=0.22 (from pmdarima)\n",
      "  Downloading scikit_learn-1.6.0-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy>=1.3.2 (from pmdarima)\n",
      "  Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting statsmodels>=0.13.2 (from pmdarima)\n",
      "  Downloading statsmodels-0.14.4-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: urllib3 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima) (2.3.0)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima) (75.6.0)\n",
      "Requirement already satisfied: packaging>=17.1 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pandas>=0.19->pmdarima) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=0.19->pmdarima)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=0.19->pmdarima)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.22->pmdarima)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting patsy>=0.5.6 (from statsmodels>=0.13.2->pmdarima)\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n",
      "Downloading pmdarima-2.0.4-cp312-cp312-win_amd64.whl (625 kB)\n",
      "   ---------------------------------------- 0.0/625.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 625.1/625.1 kB 7.9 MB/s eta 0:00:00\n",
      "Downloading Cython-3.0.11-cp312-cp312-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.8/2.8 MB 26.9 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading numpy-2.2.1-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 8.7/12.6 MB 48.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 39.5 MB/s eta 0:00:00\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Downloading scikit_learn-1.6.0-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 9.7/11.1 MB 46.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 40.8 MB/s eta 0:00:00\n",
      "Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "Downloading statsmodels-0.14.4-cp312-cp312-win_amd64.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 5.0/9.8 MB 27.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.8 MB 20.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 19.1 MB/s eta 0:00:00\n",
      "Downloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, Cython, scipy, patsy, pandas, statsmodels, scikit-learn, pmdarima\n",
      "Successfully installed Cython-3.0.11 joblib-1.4.2 numpy-2.2.1 pandas-2.2.3 patsy-1.0.1 pmdarima-2.0.4 pytz-2024.2 scikit-learn-1.6.0 scipy-1.14.1 statsmodels-0.14.4 threadpoolctl-3.5.0 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pmdarima==2.0.2\n",
      "  Downloading pmdarima-2.0.2.tar.gz (630 kB)\n",
      "     ---------------------------------------- 0.0/630.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 630.8/630.8 kB 7.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima==2.0.2) (1.4.2)\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima==2.0.2) (3.0.11)\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima==2.0.2) (2.2.1)\n",
      "Requirement already satisfied: pandas>=0.19 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima==2.0.2) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima==2.0.2) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima==2.0.2) (1.14.1)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima==2.0.2) (0.14.4)\n",
      "Requirement already satisfied: urllib3 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima==2.0.2) (2.3.0)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pmdarima==2.0.2) (75.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pandas>=0.19->pmdarima==2.0.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pandas>=0.19->pmdarima==2.0.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from pandas>=0.19->pmdarima==2.0.2) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from scikit-learn>=0.22->pmdarima==2.0.2) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from statsmodels>=0.13.2->pmdarima==2.0.2) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from statsmodels>=0.13.2->pmdarima==2.0.2) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima==2.0.2) (1.17.0)\n",
      "Building wheels for collected packages: pmdarima\n",
      "  Building wheel for pmdarima (pyproject.toml): started\n",
      "  Building wheel for pmdarima (pyproject.toml): finished with status 'error'\n",
      "Failed to build pmdarima\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for pmdarima (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [41 lines of output]\n",
      "      <string>:15: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "      Partial import of pmdarima during the build process.\n",
      "      \n",
      "      Requirements: ['joblib>=0.11\\nCython>=0.29,!=0.29.18,!=0.29.31\\nnumpy>=1.21.2\\npandas>=0.19\\nscikit-learn>=0.22\\nscipy>=1.3.2\\nstatsmodels>=0.13.2\\nurllib3\\nsetuptools>=38.6.0,!=50.0.0\\n']\n",
      "      Adding extra setuptools args\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 190, in check_package_status\n",
      "        File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2288.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "          return _bootstrap._gcd_import(name[level:], package, level)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1324, in _find_and_load_unlocked\n",
      "      ModuleNotFoundError: No module named 'numpy'\n",
      "      Traceback (most recent call last):\n",
      "        File \"d:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"d:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"d:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 251, in build_wheel\n",
      "          return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-9pg7g2am\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 438, in build_wheel\n",
      "          return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-9pg7g2am\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 426, in _build\n",
      "          return self._build_with_temp_dir(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-9pg7g2am\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 407, in _build_with_temp_dir\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-9pg7g2am\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-9pg7g2am\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 340, in <module>\n",
      "        File \"<string>\", line 327, in do_setup\n",
      "        File \"<string>\", line 210, in check_package_status\n",
      "      ImportError: numpy is not installed.\n",
      "      pmdarima requires numpy >= 1.16.\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pmdarima\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pmdarima)\n"
     ]
    }
   ],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "%pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 torch=2.1.0+cu118\n",
    "# - Update a specific package\n",
    "%pip install pmdarima -U\n",
    "# - Update a package to specific version\n",
    "%pip install --upgrade pmdarima==2.0.2\n",
    "# Note: If your environment doesn't support \"%pip install\", use \"!mamba install\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96279058-28ef-4d76-a53e-aab0f17cd0af",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "398003eb-8745-40d5-939c-71ccf86ed73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to c:\\users\\admin\\appdata\\local\\temp\\pip-req-build-56lj2yk3\n",
      "  Resolved https://github.com/huggingface/transformers to commit 05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting filelock (from transformers==4.48.0.dev0)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers==4.48.0.dev0)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from transformers==4.48.0.dev0) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from transformers==4.48.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from transformers==4.48.0.dev0) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.48.0.dev0)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from transformers==4.48.0.dev0) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.0.dev0)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.48.0.dev0)\n",
      "  Using cached safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.48.0.dev0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.48.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from requests->transformers==4.48.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from requests->transformers==4.48.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from requests->transformers==4.48.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from requests->transformers==4.48.0.dev0) (2024.12.14)\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 19.3 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.48.0.dev0-py3-none-any.whl size=10290953 sha256=5ac755a90e10ea6a3afb5228f378fb3e011b9a469e8d148f7231d50fefeffc71\n",
      "  Stored in directory: C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-hwfvdy4g\\wheels\\49\\a7\\50\\c9fdabbf10e51bb1256adb0c1a587fedd7184f5bad28d47fe3\n",
      "Successfully built transformers\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.0 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.48.0.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-req-build-56lj2yk3'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch==2.3.0\n",
      "  Downloading torch-2.3.0-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.3.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.3.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.3.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.3.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.3.0) (2024.12.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch==2.3.0)\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0)\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0)\n",
      "  Downloading tbb-2021.13.1-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Downloading torch-2.3.0-cp312-cp312-win_amd64.whl (159.7 MB)\n",
      "   ---------------------------------------- 0.0/159.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 5.2/159.7 MB 31.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 14.4/159.7 MB 39.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 18.6/159.7 MB 33.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 23.3/159.7 MB 29.5 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 25.2/159.7 MB 25.3 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 27.0/159.7 MB 22.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 28.8/159.7 MB 20.6 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 31.2/159.7 MB 19.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 33.3/159.7 MB 17.9 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 35.4/159.7 MB 17.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 37.5/159.7 MB 16.5 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 39.6/159.7 MB 16.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 42.2/159.7 MB 15.7 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 44.6/159.7 MB 15.4 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 47.2/159.7 MB 15.2 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 49.8/159.7 MB 15.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 52.4/159.7 MB 14.8 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 55.1/159.7 MB 14.6 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 57.7/159.7 MB 14.5 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 60.3/159.7 MB 14.4 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 62.9/159.7 MB 14.4 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 65.8/159.7 MB 14.3 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 68.4/159.7 MB 14.3 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 71.3/159.7 MB 14.2 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 74.4/159.7 MB 14.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 77.6/159.7 MB 14.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 80.5/159.7 MB 14.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 83.6/159.7 MB 14.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 87.0/159.7 MB 14.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 90.4/159.7 MB 14.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 93.8/159.7 MB 14.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 97.3/159.7 MB 14.5 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 100.7/159.7 MB 14.5 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 103.8/159.7 MB 14.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 107.5/159.7 MB 14.6 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 111.1/159.7 MB 14.7 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 115.1/159.7 MB 14.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 119.0/159.7 MB 14.9 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 122.7/159.7 MB 15.0 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 126.6/159.7 MB 15.1 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 130.8/159.7 MB 15.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 135.0/159.7 MB 15.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 138.9/159.7 MB 15.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 142.6/159.7 MB 15.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 146.8/159.7 MB 15.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 151.3/159.7 MB 15.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 154.9/159.7 MB 15.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.1/159.7 MB 15.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 159.7/159.7 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   ---------------------------------------- 0.0/228.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 4.2/228.5 MB 20.9 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 8.4/228.5 MB 20.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 13.1/228.5 MB 20.6 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 17.3/228.5 MB 21.0 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 21.5/228.5 MB 20.6 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 26.2/228.5 MB 20.8 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 30.9/228.5 MB 21.1 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 35.4/228.5 MB 21.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 40.1/228.5 MB 21.1 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 44.6/228.5 MB 21.2 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 49.3/228.5 MB 21.2 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 54.3/228.5 MB 21.5 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 59.8/228.5 MB 21.8 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 64.7/228.5 MB 21.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 69.7/228.5 MB 22.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 75.0/228.5 MB 22.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 80.0/228.5 MB 22.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 85.5/228.5 MB 22.3 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 90.7/228.5 MB 22.5 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 96.5/228.5 MB 22.7 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 103.0/228.5 MB 23.1 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 108.5/228.5 MB 23.2 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 115.1/228.5 MB 23.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 121.4/228.5 MB 23.9 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 127.7/228.5 MB 24.1 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 133.2/228.5 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 138.1/228.5 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 143.1/228.5 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 147.8/228.5 MB 24.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 153.6/228.5 MB 24.2 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 159.9/228.5 MB 24.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 166.5/228.5 MB 24.6 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 172.2/228.5 MB 24.6 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 176.7/228.5 MB 24.5 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 181.4/228.5 MB 24.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 186.4/228.5 MB 24.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 191.4/228.5 MB 24.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 196.3/228.5 MB 24.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 201.6/228.5 MB 24.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 206.6/228.5 MB 24.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 211.6/228.5 MB 24.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 217.1/228.5 MB 24.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 222.0/228.5 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  227.5/228.5 MB 24.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 24.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 228.5/228.5 MB 23.8 MB/s eta 0:00:00\n",
      "Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.5/3.5 MB 23.1 MB/s eta 0:00:00\n",
      "Downloading tbb-2021.13.1-py3-none-win_amd64.whl (286 kB)\n",
      "Installing collected packages: tbb, intel-openmp, mkl, torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "Successfully installed intel-openmp-2021.4.0 mkl-2021.4.0 tbb-2021.13.1 torch-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.20.1-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torchvision) (2.2.1)\n",
      "Collecting torch==2.5.1 (from torchvision)\n",
      "  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-11.0.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\course\\ibmgenai\\venv\\lib\\site-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
      "Using cached torchvision-0.20.1-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "Using cached torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "Downloading pillow-11.0.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.8/2.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.6 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.6 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.6/2.6 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.8/2.6 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.1/2.6 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pillow, torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.0\n",
      "    Uninstalling torch-2.3.0:\n",
      "      Successfully uninstalled torch-2.3.0\n",
      "Successfully installed pillow-11.0.0 torch-2.5.1 torchvision-0.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-3.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install transformers==4.40.0 \n",
    "%pip install -U git+https://github.com/huggingface/transformers\n",
    "%pip install datasets # 2.15.0\n",
    "%pip install portalocker>=2/0.0\n",
    "%pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "%pip install torch==2.3.0\n",
    "%pip install -U torchvision\n",
    "%pip install protobuf==3.20.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df41b99-0b75-4b44-83a7-e7e9c9ab0040",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended that you import all required libraries in one place (here):_\n",
    "* Note: if you got an error after running the cell below, try restarting the Kernel as some packages need a restart to be effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43c5534-cddb-4945-aad0-f0bf6bc586f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoConfig,AutoModelForCausalLM,AutoModelForSequenceClassification,BertConfig,BertForMaskedLM,TrainingArguments, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer,BertTokenizerFast,TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febffb57-144a-4c8e-abd7-b42a39b08415",
   "metadata": {},
   "source": [
    "Disable tokenizer parallelism to avoid deadlocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91015952-c3d9-4f96-a670-b7d7732529bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable TOKENIZERS_PARALLELISM to 'false'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71486cbb-edfa-4592-ba86-37f795be0de0",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39beea74-f1ce-41fe-8cea-f58ef2dbcf08",
   "metadata": {},
   "source": [
    "# Pretraining and self-supervised fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fdc0ba-15c9-41da-abd8-4d7b7d307053",
   "metadata": {},
   "source": [
    "Pretraining is a technique used in natural language processing (NLP) to train large language models (LLMs) on a vast corpus of unlabeled text data. The goal is to capture the general patterns and semantic relationships present in natural language, allowing the model to develop a deep understanding of language structure and meaning.\n",
    "\n",
    "The motivation behind pretraining transformers is to address the limitations of traditional NLP approaches that often require significant amounts of labeled data for each specific task. By leveraging the abundance of unlabeled text data, pretraining enables the model to learn fundamental language skills through self-supervised objectives, facilitating transfer learning.\n",
    "\n",
    "The pretraining objectives, such as masked language modeling (MLM) and next sentence prediction (NSP), play a crucial role in the success of transformer models. Pretrained models can be further tuned by training them on domain-specific unlabeled data, which is known as self-supervised fine-tuning.\n",
    "\n",
    "Also, the model can be fine-tuned on specific downstream tasks using labeled data, a process known as supervised fine-tuning, further improving its performance.\n",
    "\n",
    "In the following sections of this lab, you will explore pretraining objectives, loading pretrained models, data preparation, and the fine-tuning process. By the end, you will have a solid understanding of pretraining and self-supervised fine-tuning, empowering you to apply these techniques to solve real-world NLP problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4cd479-2010-4063-9c29-a4140a62f23a",
   "metadata": {},
   "source": [
    "Let's start with loading a pretrained model from Hugging Face and making an inference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c8fde6d-168b-4888-905d-19e19c4ab46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was really good. I was really surprised by how good it was.\n",
      "I was\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer)\n",
    "print(pipe(\"This movie was really\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486dcad-1928-46d5-af1c-a190dd45edcb",
   "metadata": {},
   "source": [
    "## Pre-training Objectives\n",
    "\n",
    "Pre-training objectives are crucial components of the pre-training process for transformers. These objectives define the tasks that the model is trained on during the pre-training phase, allowing it to learn meaningful contextual representations of language. Three commonly used pre-training objectives are masked language modeling (MLM), next sentence prediction (NSP) and next Ttoken prediction.\n",
    "\n",
    "1. Masked Language Modeling (MLM):\n",
    "   Masked language modeling involves randomly masking some words in a sentence and training the model to predict the masked words based on the context provided by the surrounding words(i.e., words that appear either before or after the masked word). The objective is to enable the model to learn contextual understanding and fill in missing information.\n",
    "\n",
    "2. Next Sentence Prediction (NSP):\n",
    "   Next sentence prediction involves training the model to predict whether two sentences are consecutive in the original text or randomly chosen from the corpus. This objective helps the model learn sentence-level relationships and understand the coherence between sentences.\n",
    "\n",
    "3. Next Token Prediction:\n",
    "    In this objective, the model is trained to predict the next token in a sequence of text. The model is presented with a sequence of text and must learn to predict the most likely next token based on the context.\n",
    "\n",
    "It's important to note that different pre-trained models may use variations or combinations of these objectives, depending on the specific architecture and training setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4aa9d9-bcc2-4142-a75f-f07a7e15d6e6",
   "metadata": {},
   "source": [
    "## Self-supervised training of a BERT model\n",
    "Training a BERT(Bidirectional Encoder Representations from Transformers) model is a complex and time-consuming process that requires a large corpus of unlabeled text data and significant computational resources. However, we provide you with a simplified exercise to demonstrate the steps involved in pre-training a BERT model using the Masked Language Modeling (MLM) objective.\n",
    "\n",
    "For this exercise, we'll use the Hugging Face Transformers library, which provides pre-implemented BERT models and tools for pre-training. You will be instructed to:\n",
    "- Prepare the train dataset\n",
    "- Train a Tokenizer\n",
    "- Preprocess the dataset\n",
    "- Pre-train BERT using an MLM task\n",
    "- Evaluate the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc498ad-69c9-4181-a28f-bfdb4bdf89ed",
   "metadata": {},
   "source": [
    "### Importing required datasets\n",
    "\n",
    "The WikiText dataset is a widely used benchmark dataset in the field of natural language processing (NLP). The dataset contains a large amount of text extracted from Wikipedia, which is a vast online encyclopedia covering a wide range of topics. The articles in the WikiText dataset are preprocessed to remove formatting, hyperlinks, and other metadata, resulting in a clean text corpus.\n",
    "\n",
    "The WikiText dataset has 4 different configs, and is divided into three parts: a training set, a validation set, and a test set. The training set is used for training language models, while the validation and test sets are used for evaluating the performance of the models.\n",
    "First, let's load the datasets and concatenate them together to create a big dataset.\n",
    "\n",
    "*Note: The original BERT was pretrained on Wikipedia and BookCorpus datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6b5ef69-63d3-4d42-b65c-3e166381c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 442242.74 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 1806993.64 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 939788.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cccf8a-32d1-4bf9-8ca4-01d155ba4b64",
   "metadata": {},
   "source": [
    "Let's check the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a8b8d8b-c05d-43d0-bcb7-b62df284942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e549a-fa01-4184-8a38-00de0213c0d5",
   "metadata": {},
   "source": [
    "check a sample record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19374a2d-1cdd-4017-a6a3-475c6042cae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" When Mason was injured in warm @-@ ups late in the year , Columbus was without an active goaltender on their roster . To remedy the situation , the team signed former University of Michigan goaltender Shawn Hunwick to a one @-@ day , amateur tryout contract . After being eliminated from the NCAA Tournament just days prior , Hunwick skipped an astronomy class and drove his worn down 2003 Ford Ranger to Columbus to make the game . He served as the back @-@ up to Allen York during the game , and the following day , he signed a contract for the remainder of the year . With Mason returning from injury , Hunwick was third on the team 's depth chart when an injury to York allowed Hunwick to remain as the back @-@ up for the final two games of the year . In the final game of the season , the Blue Jackets were leading the Islanders 7 – 3 with 2 : 33 remaining when , at the behest of his teammates , Head Coach Todd Richards put Hunwick in to finish the game . He did not face a shot . Hunwick was the franchise record ninth player to make his NHL debut during the season . Conversely , Vaclav Prospal played in his 1,000th NHL game during the year . \\n\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check a sample record\n",
    "dataset[\"train\"][400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe2a2b-3674-48df-871e-5e72885076a4",
   "metadata": {},
   "source": [
    "This dataset contains 36,718 rows of training data. If you do not run the code on a GPU-powered notebook, you will need to decrease the size of dataset to be able to complete the training. You can uncomment the commands below to select a desired section of dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9597b63b-769c-450c-a080-0253d3f11b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(1000)])\n",
    "dataset[\"test\"] = dataset[\"test\"].select([i for i in range(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde0aa7-2562-40d5-9661-5d523bae1d7b",
   "metadata": {},
   "source": [
    "Below files are next used in creating TextDataset objects for the training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c759cf5-f5ec-4a7a-9f18-fc15ca972599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the datasets to text files\n",
    "output_file_train = \"wikitext_dataset_train.txt\"\n",
    "output_file_test = \"wikitext_dataset_test.txt\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_train, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Iterate over each example in the dataset\n",
    "    for example in dataset[\"train\"]:\n",
    "        # Write the example text to the file\n",
    "        f.write(example[\"text\"] + \"\\n\")\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file_test, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Iterate over each example in the dataset\n",
    "    for example in dataset[\"test\"]:\n",
    "        # Write the example text to the file\n",
    "        f.write(example[\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea3f4a-cb85-4732-85dd-5a1bc35fec0d",
   "metadata": {},
   "source": [
    "You need to define a tokenizer to be used for tokenizing the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d254c612-feca-48ab-9426-69d2252f1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer from existing one to re-use special tokens\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bfaf4c2-9c08-472a-a96c-2d9bbd217ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, is_decoder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8021ad1-9574-42d8-b619-eaa7cae5d6c8",
   "metadata": {},
   "source": [
    "### Training a Tokenizer(Optional)\n",
    "\n",
    "In the previous cell, you created an instance of tokenizer from a pre-trained BERT tokenizer. If you want to train the tokenizer on your own dataset, you can uncomment the code below. This is specially helpful when using transformers for specific areas such as medicine where tokens are somehow different than the general tokens that tokenizers are created based on. (You can skip this step if you do not want to train the tokenizer on your specific data):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34898d55-0818-4fd4-b967-4f9ca52c72db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.09it/s]\n"
     ]
    }
   ],
   "source": [
    "## create a python generator to dynamically load the data\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        yield dataset['train'][i : i + batch_size][\"text\"]\n",
    "\n",
    "## create a tokenizer from existing one to re-use special tokens\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "## train the tokenizer using our own dataset\n",
    "bert_tokenizer = bert_tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=30522)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49c094-0788-4094-b94a-243314c49b8d",
   "metadata": {},
   "source": [
    "### Pretraining\n",
    "\n",
    "In this step, we define the configuration of the BERT model and create the model:\n",
    "#### Define the BERT Configuration\n",
    "Here, we define the configuration settings for a BERT model using `BertConfig`. This includes setting various parameters related to the model's architecture:\n",
    "- **vocab_size=30522**: Specifies the size of the vocabulary. This number should match the vocabulary size used by the tokenizer.\n",
    "- **hidden_size=768**: Sets the size of the hidden layers.\n",
    "- **num_hidden_layers=12**: Determines the number of hidden layers in the transformer model.\n",
    "- **num_attention_heads=12**: Sets the number of attention heads in each attention layer.\n",
    "- **intermediate_size=3072**: Specifies the size of the \"intermediate\" (i.e., feed-forward) layer within the transformer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e45af23e-59b0-4c7f-b1ec-50c05fcd6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BERT configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=len(bert_tokenizer.get_vocab()),  # Specify the vocabulary size(Make sure this number equals the vocab_size of the tokenizer)\n",
    "    hidden_size=768,  # Set the hidden size\n",
    "    num_hidden_layers=12,  # Set the number of layers\n",
    "    num_attention_heads=12,  # Set the number of attention heads\n",
    "    intermediate_size=3072,  # Set the intermediate size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a6583-5635-46b5-934e-087670694b69",
   "metadata": {},
   "source": [
    " Create the BERT model for pre-training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f40db45e-6ab1-4f50-93bc-537517929a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "# Create the BERT model for pre-training\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd5870-16c8-450e-ba3d-7cf78b52f3b9",
   "metadata": {},
   "source": [
    "check model configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42e6aea1-759e-4ccd-ac7d-0051640c85f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(12575, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=12575, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model configuration\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc25229-7e8e-4d26-8975-45d633b91d16",
   "metadata": {},
   "source": [
    "### Define the Training Dataset\n",
    "Here, we define a training dataset using the `TextDataset` class, which is suited for loading and processing text data for training language models. This setup typically involves a few key parameters:\n",
    "\n",
    "- **tokenizer=bert_tokenizer**: Specifies the tokenizer to be used. Here, `bert_tokenizer` is an instance of a BERT tokenizer, responsible for converting text into tokens that the model can understand.\n",
    "- **file_path=\"wikitext_dataset_train.txt\"**: The path to the pre-training data file. This should point to a text file containing the training data.\n",
    "- **block_size=128**: Sets the desired block size for training. This defines the length of the sequences that the model will be trained on\n",
    "\n",
    "The `TextDataset` class is designed to take large pieces of text (such as those found in the specified file), tokenize them, and efficiently handle them in manageable blocks of the specified size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45056c31-3fb9-4c5c-b988-62b9598d7068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (55468 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Prepare the pre-training data as a TextDataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    file_path=\"wikitext_dataset_train.txt\",  # Path to your pre-training data file\n",
    "    block_size=128  # Set the desired block size for training\n",
    ")\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    file_path=\"wikitext_dataset_test.txt\",  # Path to your pre-training data file\n",
    "    block_size=128  # Set the desired block size for training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d421365-4e9a-4437-bc16-de163fb3d72e",
   "metadata": {},
   "source": [
    "examining  one sample the token indexes  are shown here with the block size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "604f3e82-1f5a-4617-8ed2-2f6b9d1ba46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,    31,   619,   799,  1414,    31,  3541,   593,   619,    22,\n",
       "           29, 12314,   799,    12,  3189,    29,   105,   103,  5412,    15,\n",
       "          490,    17,   619,   184,   171,  2553,    22,    13,    15,  3533,\n",
       "         3584,   191,   216,   619,   799,  1414,  1540,  2033,    15,   256,\n",
       "           35,  5441,  1124,    32,    16,    32,  1843,  2188,   398,  1762,\n",
       "          251,  3402,   188,  3160,    17,  2434,   209,   171,  2290,  4153,\n",
       "           17,  1349,   180,  1685,  1213,   180,  2033,    15,   288,   256,\n",
       "          171,  1310,   398,   180,   171,   619,  1220,    17,  6800,   171,\n",
       "          859,  7536,   184,  5441,   188,  1151,    32,    16,    32,   533,\n",
       "         1972,   216,   408,  4372,    15,   171,  1504, 10328,  9719,   191,\n",
       "          171,   447,   398,   188,  6353,   171,     6,  1864,     6,    15,\n",
       "           35,  3527,  1062,  2489,  4113,   171,  1807,   184,  3575,   514,\n",
       "          171,   753,  7207,   527,   407,   852,  2471,     3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2cc71c-c5d0-4c83-a616-2f9fc4529dc6",
   "metadata": {},
   "source": [
    "Then, we prepare data for the MLM task (masking random tokens):\n",
    "### Define the Data Collator for Language Modeling\n",
    "This line of code sets up a `DataCollatorForLanguageModeling` from the Hugging Face Transformers library. A data collator is used during training to dynamically create batches of data. For language modeling, particularly for models like BERT that use masked language modeling (MLM), this collator prepares training batches by automatically masking tokens according to a specified probability. Here are the details of the parameters used:\n",
    "\n",
    "- **tokenizer=bert_tokenizer**: Specifies the tokenizer to be used with the data collator. The `bert_tokenizer` is responsible for tokenizing the text and converting it to the format expected by the model.\n",
    "- **mlm=True**: Indicates that the data collator should mask tokens for masked language modeling training. This parameter being set to `True` configures the collator to randomly mask some of the tokens in the input data, which the model will then attempt to predict.\n",
    "- **mlm_probability=0.15**: Sets the probability with which tokens will be masked. A probability of 0.15 means that, on average, 15% of the tokens in any sequence will be replaced with a mask token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fc0e0e7-df29-4592-94d7-f5cdd09fb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=bert_tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7c60d65-3f76-4160-91eb-79f726f11ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,    31,   619,  2130,  1414,    31,  3541,     4,   619,    22,\n",
       "             29,     4,   799,    12,  3189,    29,     4,   103,  5412,    15,\n",
       "            490,     4,   619,   184,   171,  2553,    22,    13,    15,  3533,\n",
       "           3584,   191,     4,   619,   799,  1414,  1540,  2033,    15,   256,\n",
       "             35,  5441,  1124,    32,    16,     4,  1843,  2188,   398,  1762,\n",
       "            251,  3402,     4,     4,    17,  2434,   209,   171,  2290,  4153,\n",
       "             17,  1349,   180,  1685,  1213,   180,  2033,    15,     4,   256,\n",
       "            171,  1310,   398,   180,     4,   619,  1220,    17,  6800,   171,\n",
       "            859,  7536,   184,     4,   188,  1151,    32,    16,  1662,   533,\n",
       "           1972,   216,   408,  4372,    15,   171,  1504, 10328,  9719,   191,\n",
       "            171,   447,   398,   188,  6353,   171,     6,  1864,     6,    15,\n",
       "             35,  3527,  1062,  2489,  4113,   171,  1807,   184,  3575,   514,\n",
       "            171,     4,     4,   527,   407,   852,  2471,     3]]),\n",
       " 'labels': tensor([[ -100,  -100,  -100,   799,  -100,  -100,  -100,   593,  -100,  -100,\n",
       "           -100, 12314,  -100,  -100,  -100,  -100,   105,  -100,  -100,  -100,\n",
       "           -100,    17,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,   216,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,    32,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,   188,  3160,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   288,  -100,\n",
       "           -100,  -100,  -100,  -100,   171,   619,  -100,    17,  -100,  -100,\n",
       "           -100,  -100,  -100,  5441,  -100,  -100,  -100,  -100,    32,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   753,  7207,   527,  -100,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how collator transforms a sample input data record\n",
    "data_collator([train_dataset[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606da1c8-23ee-482b-b442-b87896920ef9",
   "metadata": {},
   "source": [
    "Now, we train the BERT Model using the Trainer module. (For a complete list of training arguments, check [here](https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/trainer#transformers.TrainingArguments)):\n",
    "This section configures the training process by specifying various parameters that control how the model is trained, evaluated, and saved:\n",
    "\n",
    "- **output_dir=\"./trained_model\"**: Specifies the directory where the trained model and other output files will be saved.\n",
    "- **overwrite_output_dir=True**: If set to `True`, this will overwrite the contents of the output directory if it already exists. This is useful when running experiments multiple times.\n",
    "- **do_eval=True**: Enables evaluation of the model. If `True`, the model will be evaluated at the specified intervals.\n",
    "- **evaluation_strategy=\"epoch\"**: Defines when the model should be evaluated. Setting this to \"epoch\" means the model will be evaluated at the end of each epoch.\n",
    "- **learning_rate=5e-5**: Sets the learning rate for training the model. This is a typical learning rate for fine-tuning BERT-like models.\n",
    "- **num_train_epochs=10**: Specifies the number of training epochs. Each epoch involves a full pass over the training data.\n",
    "- **per_device_train_batch_size=2**: Sets the batch size for training on each device. This should be set based on the memory capacity of your hardware.\n",
    "- **save_total_limit=2**: Limits the total number of model checkpoints to be saved. Only the most recent two checkpoints will be kept.\n",
    "- **logging_steps=20**: Determines how often to log training information, which can help monitor the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46bf8e8c-5354-4030-aadc-6627919bbc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2200 [00:13<23:37,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.6854, 'grad_norm': 7.9176154136657715, 'learning_rate': 4.9545454545454553e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 40/2200 [00:26<23:09,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.7963, 'grad_norm': 6.877941608428955, 'learning_rate': 4.909090909090909e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 60/2200 [00:39<22:34,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.672, 'grad_norm': 6.9616265296936035, 'learning_rate': 4.863636363636364e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 80/2200 [00:52<22:51,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2586, 'grad_norm': 6.833195209503174, 'learning_rate': 4.8181818181818186e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 100/2200 [01:05<22:34,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.1587, 'grad_norm': 7.211719036102295, 'learning_rate': 4.772727272727273e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 120/2200 [01:18<21:56,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2264, 'grad_norm': 6.316149711608887, 'learning_rate': 4.7272727272727275e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 140/2200 [01:31<21:38,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.1101, 'grad_norm': 5.497359275817871, 'learning_rate': 4.681818181818182e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 160/2200 [01:43<21:08,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0734, 'grad_norm': 6.176764488220215, 'learning_rate': 4.636363636363636e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 180/2200 [01:56<21:19,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9881, 'grad_norm': 6.242911338806152, 'learning_rate': 4.5909090909090914e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 200/2200 [02:08<20:49,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9933, 'grad_norm': 5.604017734527588, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 220/2200 [02:21<20:34,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.1042, 'grad_norm': 5.359533309936523, 'learning_rate': 4.5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 220/2200 [02:25<20:34,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.898343086242676, 'eval_runtime': 4.4645, 'eval_samples_per_second': 23.071, 'eval_steps_per_second': 2.912, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 240/2200 [02:38<21:52,  1.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8234, 'grad_norm': 5.851937770843506, 'learning_rate': 4.454545454545455e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 260/2200 [02:51<20:43,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8225, 'grad_norm': 6.21433162689209, 'learning_rate': 4.409090909090909e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 280/2200 [03:04<20:27,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8061, 'grad_norm': 4.921005725860596, 'learning_rate': 4.3636363636363636e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 300/2200 [03:17<20:05,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9762, 'grad_norm': 5.653116703033447, 'learning_rate': 4.318181818181819e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 320/2200 [03:29<19:23,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.1415, 'grad_norm': 5.1109843254089355, 'learning_rate': 4.2727272727272724e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 340/2200 [03:42<19:05,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9506, 'grad_norm': 5.632959365844727, 'learning_rate': 4.2272727272727275e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 360/2200 [03:54<19:08,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7768, 'grad_norm': 4.918776035308838, 'learning_rate': 4.181818181818182e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 380/2200 [04:07<19:30,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7739, 'grad_norm': 5.500741004943848, 'learning_rate': 4.1363636363636364e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 400/2200 [04:20<18:22,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0603, 'grad_norm': 5.681009292602539, 'learning_rate': 4.0909090909090915e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 420/2200 [04:32<18:55,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9691, 'grad_norm': 4.311225891113281, 'learning_rate': 4.045454545454546e-05, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 440/2200 [04:44<17:09,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8034, 'grad_norm': 5.501510143280029, 'learning_rate': 4e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 440/2200 [04:49<17:09,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.988618850708008, 'eval_runtime': 4.2869, 'eval_samples_per_second': 24.027, 'eval_steps_per_second': 3.032, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 460/2200 [05:01<17:41,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0108, 'grad_norm': 5.463977336883545, 'learning_rate': 3.954545454545455e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 480/2200 [05:13<16:39,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8901, 'grad_norm': 5.646495819091797, 'learning_rate': 3.909090909090909e-05, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 500/2200 [05:25<17:03,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8794, 'grad_norm': 4.787571907043457, 'learning_rate': 3.8636363636363636e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 520/2200 [05:42<16:38,  1.68it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7924, 'grad_norm': 5.253170013427734, 'learning_rate': 3.818181818181819e-05, 'epoch': 2.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 540/2200 [05:54<16:41,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.911, 'grad_norm': 4.960644721984863, 'learning_rate': 3.7727272727272725e-05, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 560/2200 [06:06<16:17,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8957, 'grad_norm': 5.300518989562988, 'learning_rate': 3.7272727272727276e-05, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 580/2200 [06:18<16:38,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9509, 'grad_norm': 5.891037464141846, 'learning_rate': 3.681818181818182e-05, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 600/2200 [06:31<17:22,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8585, 'grad_norm': 4.70629358291626, 'learning_rate': 3.6363636363636364e-05, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 620/2200 [06:43<15:31,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0128, 'grad_norm': 5.0897536277771, 'learning_rate': 3.590909090909091e-05, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 640/2200 [06:55<15:08,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9196, 'grad_norm': 5.418538570404053, 'learning_rate': 3.545454545454546e-05, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 660/2200 [07:07<14:44,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7506, 'grad_norm': 5.332900047302246, 'learning_rate': 3.5e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 30%|███       | 660/2200 [07:11<14:44,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.077054023742676, 'eval_runtime': 4.406, 'eval_samples_per_second': 23.377, 'eval_steps_per_second': 2.951, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 680/2200 [07:23<14:42,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8501, 'grad_norm': 5.486556053161621, 'learning_rate': 3.454545454545455e-05, 'epoch': 3.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 700/2200 [07:35<14:31,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7417, 'grad_norm': 5.315493583679199, 'learning_rate': 3.409090909090909e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 720/2200 [07:46<14:27,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0218, 'grad_norm': 5.296100616455078, 'learning_rate': 3.3636363636363636e-05, 'epoch': 3.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 740/2200 [07:58<13:57,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7724, 'grad_norm': 4.756875991821289, 'learning_rate': 3.318181818181819e-05, 'epoch': 3.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 760/2200 [08:09<13:40,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7015, 'grad_norm': 5.597079277038574, 'learning_rate': 3.272727272727273e-05, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 780/2200 [08:21<13:32,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9017, 'grad_norm': 4.529539108276367, 'learning_rate': 3.2272727272727276e-05, 'epoch': 3.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 800/2200 [08:32<13:17,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9803, 'grad_norm': 5.777277946472168, 'learning_rate': 3.181818181818182e-05, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 820/2200 [08:44<13:04,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.946, 'grad_norm': 5.125247478485107, 'learning_rate': 3.1363636363636365e-05, 'epoch': 3.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 840/2200 [08:55<12:58,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7627, 'grad_norm': 4.698935031890869, 'learning_rate': 3.090909090909091e-05, 'epoch': 3.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 860/2200 [09:07<12:41,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.792, 'grad_norm': 4.561478614807129, 'learning_rate': 3.0454545454545456e-05, 'epoch': 3.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 880/2200 [09:18<12:42,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8872, 'grad_norm': 5.3332133293151855, 'learning_rate': 3e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|████      | 880/2200 [09:23<12:42,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.16103458404541, 'eval_runtime': 4.3984, 'eval_samples_per_second': 23.418, 'eval_steps_per_second': 2.956, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 900/2200 [09:35<12:34,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8564, 'grad_norm': 5.353549957275391, 'learning_rate': 2.954545454545455e-05, 'epoch': 4.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 920/2200 [09:46<12:24,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6681, 'grad_norm': 4.911650657653809, 'learning_rate': 2.909090909090909e-05, 'epoch': 4.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 940/2200 [09:58<12:15,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9156, 'grad_norm': 5.606752872467041, 'learning_rate': 2.863636363636364e-05, 'epoch': 4.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 960/2200 [10:10<12:01,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9168, 'grad_norm': 4.91315221786499, 'learning_rate': 2.818181818181818e-05, 'epoch': 4.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 980/2200 [10:21<11:50,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.92, 'grad_norm': 5.263175010681152, 'learning_rate': 2.772727272727273e-05, 'epoch': 4.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 1000/2200 [10:33<11:44,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.811, 'grad_norm': 4.652979373931885, 'learning_rate': 2.7272727272727273e-05, 'epoch': 4.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 1020/2200 [10:49<11:26,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.915, 'grad_norm': 4.310720443725586, 'learning_rate': 2.681818181818182e-05, 'epoch': 4.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1040/2200 [11:01<11:15,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8852, 'grad_norm': 4.993492603302002, 'learning_rate': 2.636363636363636e-05, 'epoch': 4.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1060/2200 [11:12<11:06,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8186, 'grad_norm': 4.899363994598389, 'learning_rate': 2.590909090909091e-05, 'epoch': 4.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 1080/2200 [11:24<11:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8041, 'grad_norm': 5.355813980102539, 'learning_rate': 2.5454545454545454e-05, 'epoch': 4.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1100/2200 [11:36<10:40,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9561, 'grad_norm': 5.472437858581543, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 1100/2200 [11:40<10:40,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.20773696899414, 'eval_runtime': 4.3987, 'eval_samples_per_second': 23.416, 'eval_steps_per_second': 2.955, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1120/2200 [11:52<10:24,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7878, 'grad_norm': 5.413906574249268, 'learning_rate': 2.4545454545454545e-05, 'epoch': 5.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1140/2200 [12:04<10:21,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6887, 'grad_norm': 5.311718463897705, 'learning_rate': 2.4090909090909093e-05, 'epoch': 5.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1160/2200 [12:15<09:54,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8721, 'grad_norm': 4.6929731369018555, 'learning_rate': 2.3636363636363637e-05, 'epoch': 5.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 1180/2200 [12:27<09:56,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8393, 'grad_norm': 4.819812774658203, 'learning_rate': 2.318181818181818e-05, 'epoch': 5.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1200/2200 [12:39<09:30,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.6725, 'grad_norm': 5.260977745056152, 'learning_rate': 2.272727272727273e-05, 'epoch': 5.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1220/2200 [12:50<09:17,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0802, 'grad_norm': 6.328892230987549, 'learning_rate': 2.2272727272727274e-05, 'epoch': 5.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 1240/2200 [13:02<09:05,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6772, 'grad_norm': 4.880925178527832, 'learning_rate': 2.1818181818181818e-05, 'epoch': 5.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1260/2200 [13:13<08:52,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8621, 'grad_norm': 5.449679374694824, 'learning_rate': 2.1363636363636362e-05, 'epoch': 5.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1280/2200 [13:25<08:44,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5471, 'grad_norm': 6.194889068603516, 'learning_rate': 2.090909090909091e-05, 'epoch': 5.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 1300/2200 [13:36<08:32,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7196, 'grad_norm': 6.796223163604736, 'learning_rate': 2.0454545454545457e-05, 'epoch': 5.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1320/2200 [13:48<08:22,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6819, 'grad_norm': 5.312436580657959, 'learning_rate': 2e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 1320/2200 [13:52<08:22,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.239398002624512, 'eval_runtime': 4.3982, 'eval_samples_per_second': 23.418, 'eval_steps_per_second': 2.956, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 1340/2200 [14:04<08:29,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7874, 'grad_norm': 5.362356662750244, 'learning_rate': 1.9545454545454546e-05, 'epoch': 6.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1360/2200 [14:16<08:04,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8024, 'grad_norm': 5.021642208099365, 'learning_rate': 1.9090909090909094e-05, 'epoch': 6.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1380/2200 [14:27<08:01,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6418, 'grad_norm': 4.746716022491455, 'learning_rate': 1.8636363636363638e-05, 'epoch': 6.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 1400/2200 [14:39<07:45,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7071, 'grad_norm': 5.107680797576904, 'learning_rate': 1.8181818181818182e-05, 'epoch': 6.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 1420/2200 [14:51<07:33,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.855, 'grad_norm': 4.9152350425720215, 'learning_rate': 1.772727272727273e-05, 'epoch': 6.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1440/2200 [15:02<07:23,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7346, 'grad_norm': 4.554744243621826, 'learning_rate': 1.7272727272727274e-05, 'epoch': 6.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 1460/2200 [15:14<07:11,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8105, 'grad_norm': 5.873269557952881, 'learning_rate': 1.6818181818181818e-05, 'epoch': 6.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1480/2200 [15:26<06:56,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7383, 'grad_norm': 5.49183988571167, 'learning_rate': 1.6363636363636366e-05, 'epoch': 6.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1500/2200 [15:38<06:43,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8463, 'grad_norm': 5.053542137145996, 'learning_rate': 1.590909090909091e-05, 'epoch': 6.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 1520/2200 [15:54<06:38,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6826, 'grad_norm': 5.023460388183594, 'learning_rate': 1.5454545454545454e-05, 'epoch': 6.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1540/2200 [16:06<06:22,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6167, 'grad_norm': 5.70521354675293, 'learning_rate': 1.5e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|███████   | 1540/2200 [16:10<06:22,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.363091468811035, 'eval_runtime': 4.3353, 'eval_samples_per_second': 23.758, 'eval_steps_per_second': 2.999, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1560/2200 [16:22<06:14,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8568, 'grad_norm': 6.6288018226623535, 'learning_rate': 1.4545454545454545e-05, 'epoch': 7.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1580/2200 [16:34<05:56,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8552, 'grad_norm': 5.850818157196045, 'learning_rate': 1.409090909090909e-05, 'epoch': 7.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1600/2200 [16:46<05:53,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8497, 'grad_norm': 5.369394779205322, 'learning_rate': 1.3636363636363637e-05, 'epoch': 7.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 1620/2200 [16:57<05:38,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7611, 'grad_norm': 5.115549087524414, 'learning_rate': 1.318181818181818e-05, 'epoch': 7.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 1640/2200 [17:09<05:29,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7687, 'grad_norm': 5.412525653839111, 'learning_rate': 1.2727272727272727e-05, 'epoch': 7.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1660/2200 [17:21<05:15,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6191, 'grad_norm': 4.506237506866455, 'learning_rate': 1.2272727272727273e-05, 'epoch': 7.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 1680/2200 [17:33<05:03,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8867, 'grad_norm': 4.6405415534973145, 'learning_rate': 1.1818181818181819e-05, 'epoch': 7.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1700/2200 [17:44<04:49,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.707, 'grad_norm': 4.9450860023498535, 'learning_rate': 1.1363636363636365e-05, 'epoch': 7.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1720/2200 [17:56<04:34,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7947, 'grad_norm': 6.156165599822998, 'learning_rate': 1.0909090909090909e-05, 'epoch': 7.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 1740/2200 [18:07<04:20,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8575, 'grad_norm': 4.857022285461426, 'learning_rate': 1.0454545454545455e-05, 'epoch': 7.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1760/2200 [18:19<04:10,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8409, 'grad_norm': 4.812510013580322, 'learning_rate': 1e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 1760/2200 [18:23<04:10,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.23674488067627, 'eval_runtime': 4.4492, 'eval_samples_per_second': 23.15, 'eval_steps_per_second': 2.922, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1780/2200 [18:35<04:07,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.883, 'grad_norm': 5.015549659729004, 'learning_rate': 9.545454545454547e-06, 'epoch': 8.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1800/2200 [18:47<03:51,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6765, 'grad_norm': 4.690027713775635, 'learning_rate': 9.090909090909091e-06, 'epoch': 8.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1820/2200 [18:59<03:49,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7644, 'grad_norm': 4.659084796905518, 'learning_rate': 8.636363636363637e-06, 'epoch': 8.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 1840/2200 [19:11<03:27,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6485, 'grad_norm': 4.568099498748779, 'learning_rate': 8.181818181818183e-06, 'epoch': 8.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 1860/2200 [19:22<03:16,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6408, 'grad_norm': 5.527541637420654, 'learning_rate': 7.727272727272727e-06, 'epoch': 8.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 1880/2200 [19:34<03:04,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5976, 'grad_norm': 5.466050624847412, 'learning_rate': 7.272727272727272e-06, 'epoch': 8.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 1900/2200 [19:46<02:54,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8336, 'grad_norm': 4.779109954833984, 'learning_rate': 6.818181818181818e-06, 'epoch': 8.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1920/2200 [19:57<02:43,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8949, 'grad_norm': 4.8586506843566895, 'learning_rate': 6.363636363636363e-06, 'epoch': 8.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1940/2200 [20:09<02:32,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7763, 'grad_norm': 5.319136142730713, 'learning_rate': 5.909090909090909e-06, 'epoch': 8.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 1960/2200 [20:21<02:20,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9531, 'grad_norm': 6.259253025054932, 'learning_rate': 5.4545454545454545e-06, 'epoch': 8.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1980/2200 [20:32<02:08,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9334, 'grad_norm': 4.414152145385742, 'learning_rate': 5e-06, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 1980/2200 [20:37<02:08,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.381245613098145, 'eval_runtime': 4.3701, 'eval_samples_per_second': 23.569, 'eval_steps_per_second': 2.975, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 2000/2200 [20:49<01:55,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.4592, 'grad_norm': 6.558801174163818, 'learning_rate': 4.5454545454545455e-06, 'epoch': 9.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 2020/2200 [21:07<01:47,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8006, 'grad_norm': 5.29049825668335, 'learning_rate': 4.0909090909090915e-06, 'epoch': 9.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2040/2200 [21:19<01:31,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8004, 'grad_norm': 5.10560417175293, 'learning_rate': 3.636363636363636e-06, 'epoch': 9.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 2060/2200 [21:30<01:22,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6757, 'grad_norm': 5.200350284576416, 'learning_rate': 3.1818181818181817e-06, 'epoch': 9.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 2080/2200 [21:42<01:10,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8404, 'grad_norm': 5.327524185180664, 'learning_rate': 2.7272727272727272e-06, 'epoch': 9.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 2100/2200 [21:54<00:57,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8274, 'grad_norm': 4.971123695373535, 'learning_rate': 2.2727272727272728e-06, 'epoch': 9.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 2120/2200 [22:06<00:45,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5952, 'grad_norm': 4.928767204284668, 'learning_rate': 1.818181818181818e-06, 'epoch': 9.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2140/2200 [22:17<00:34,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9341, 'grad_norm': 5.303535461425781, 'learning_rate': 1.3636363636363636e-06, 'epoch': 9.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 2160/2200 [22:29<00:23,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7201, 'grad_norm': 5.099649906158447, 'learning_rate': 9.09090909090909e-07, 'epoch': 9.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 2180/2200 [22:41<00:11,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8119, 'grad_norm': 5.022861003875732, 'learning_rate': 4.545454545454545e-07, 'epoch': 9.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2200/2200 [22:52<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8374, 'grad_norm': 4.74929666519165, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 2200/2200 [23:02<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.378961563110352, 'eval_runtime': 4.4166, 'eval_samples_per_second': 23.321, 'eval_steps_per_second': 2.943, 'epoch': 10.0}\n",
      "{'train_runtime': 1382.3563, 'train_samples_per_second': 3.183, 'train_steps_per_second': 1.591, 'train_loss': 6.8910849137739705, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2200, training_loss=6.8910849137739705, metrics={'train_runtime': 1382.3563, 'train_samples_per_second': 3.183, 'train_steps_per_second': 1.591, 'total_flos': 289464647577600.0, 'train_loss': 6.8910849137739705, 'epoch': 10.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_model\",  # Specify the output directory for the trained model\n",
    "    overwrite_output_dir=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,  # Specify the number of training epochs\n",
    "    per_device_train_batch_size=2,  # Set the batch size for training\n",
    "    save_total_limit=2,  # Limit the total number of saved checkpoints\n",
    "    logging_steps = 20\n",
    "    \n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Start the pre-training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f467e-e974-4b5e-852a-f0d574eb60bb",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance\n",
    "\n",
    "Let's check the performance of the trained model. Perplexity is commonly used to compare different language models or different configurations of the same model.\n",
    "After training, perplexity can be calculated on a held-out evaluation dataset to assess the model's performance. The perplexity is calculated by feeding the evaluation dataset through the model and comparing the predicted probabilities of the target tokens with the actual token values that are masked.\n",
    "\n",
    "A lower perplexity score indicates that the model has a better understanding of the language and is more effective at predicting the masked tokens. It suggests that the model has learned useful representations and can generalize well to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bce24b9-d6e4-4cae-a1d0-9c8ae57bbc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3906.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a4a1e-f18d-4f06-9cbb-a47dab650960",
   "metadata": {},
   "source": [
    "## Loading the saved model\n",
    "If you want to skip training and load the model that you trained for 10 epochs, go ahead and uncomment the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ee6c3-1fc5-4584-b153-e2335abe21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BeXRxFT2EyQAmBHvxVaMYQ/bert-scratch-model.pt'\n",
    "model.load_state_dict(torch.load('bert-scratch-model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88218f-2bab-4878-9a77-eaa2fdd615b9",
   "metadata": {},
   "source": [
    "The simplest way to try out the model for inference is to use it in a pipeline(). Instantiate a pipeline for fill-mask with your model, and pass your text to it. If you like, you can use the top_k parameter to specify how many predictions to return:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1a481b6-9d42-4571-b388-44c608790887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: the, Confidence: 0.06\n",
      "Predicted token: ,, Confidence: 0.05\n",
      "Predicted token: ., Confidence: 0.04\n",
      "Predicted token: of, Confidence: 0.03\n",
      "Predicted token: and, Confidence: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Define the input text with a masked token\n",
    "text = \"This is a [MASK] movie!\"\n",
    "\n",
    "# Create a pipeline for the \"fill-mask\" task\n",
    "mask_filler = pipeline(\"fill-mask\", model=model,tokenizer=bert_tokenizer)\n",
    "\n",
    "# Generate predictions by filling the mask in the input text\n",
    "results = mask_filler(text) #top_k parameter can be set \n",
    "\n",
    "# Print the predicted sequences\n",
    "for result in results:\n",
    "    print(f\"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3c1a4-90b0-4d96-a49c-461726b4373b",
   "metadata": {},
   "source": [
    "You can see that [MASK] is replaced by the most frequent token. This weak performance can be due to insufficient training, lack of training data, model architecture, or not tuning hyperparameters. Let's try a pretrained model from Hugging Face:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff36807-2a3d-4cae-ba72-959546612b6b",
   "metadata": {},
   "source": [
    "## Inferencing a pretrained BERT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8981a2c6-9c4e-40a5-8fc8-3f2a4af11eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: great, Confidence: 0.16\n",
      "Predicted token: horror, Confidence: 0.08\n",
      "Predicted token: good, Confidence: 0.08\n",
      "Predicted token: bad, Confidence: 0.05\n",
      "Predicted token: fantastic, Confidence: 0.04\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained BERT model and tokenizer\n",
    "pretrained_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "pretrained_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the input text with a masked token\n",
    "text = \"This is a [MASK] movie!\"\n",
    "\n",
    "# Create the pipeline\n",
    "mask_filler = pipeline(task='fill-mask', model=pretrained_model,tokenizer=pretrained_tokenizer)\n",
    "\n",
    "# Perform inference using the pipeline\n",
    "results = mask_filler(text)\n",
    "for result in results:\n",
    "    print(f\"Predicted token: {result['token_str']}, Confidence: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d3d31-f070-481b-8f9b-09f051fa4187",
   "metadata": {},
   "source": [
    "This pretrianed model performs way better than the model you just trained for a few epochs using a single dataset. Still, pretrained models cannot be used for specific tasks, such as sentiment extraction or sequence classification. This is why supervised fine-tuning methods are introduced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58efa31-a550-475d-b476-88ac38989672",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f07dfa-4389-4390-8709-a8f5d6e9e688",
   "metadata": {},
   "source": [
    "## Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9787cb-67e0-43b9-8516-3901dfe94f9f",
   "metadata": {},
   "source": [
    "1. Create a model and tokenizer using Hugging Face library.\n",
    "2. Go to this [link](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=trending)\n",
    "3. Choose a Text Classification dataset that you can load, for instance 'stanfordnlp/snli'\n",
    "4. Use that dataset to train your model(please be mindful of the resources available for the training) and evaluate it.\n",
    "\n",
    "   >Note: The lab environment doesn't have the resources to support the training and this might cause the kernel to die.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2f0ed-14ac-43d1-a6e4-b0ce0697dfa7",
   "metadata": {},
   "source": [
    "<details><summary>Click here for a hint</summary>\n",
    "\n",
    "-   SNLI has 3 labels\n",
    "-   You can use `load_dataset(\"stanfordnlp/snli\")` to load the dataset\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b571d-4223-4133-906e-e7d1a56668c1",
   "metadata": {},
   "source": [
    "<details><summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SNLI dataset\n",
    "snli = load_dataset(\"stanfordnlp/snli\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "  premise = examples[\"premise\"]\n",
    "  hypothesis = examples[\"hypothesis\"]\n",
    "  return tokenizer(premise, hypothesis, padding=\"max_length\", truncation=True)\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Apply preprocessing to training and validation sets\n",
    "train_encoded = snli[\"train\"].map(preprocess_function, batched=True)\n",
    "val_encoded = snli[\"validation\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# Training function (replace with your training loop)\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Replace with your output directory\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_encoded,\n",
    "    eval_dataset=val_encoded,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluation function (replace with your metrics)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions, labels = trainer.predict(val_encoded)\n",
    "accuracy = accuracy_score(labels, predictions.argmax(-1))\n",
    "print(f\"Accuracy on validation set: {accuracy:.4f}\")\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b0c2428-c6c7-45b1-b1fe-5fd8d0317d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 1325507.70 examples/s]\n",
      "Generating validation split: 100%|██████████| 10000/10000 [00:00<00:00, 1999286.91 examples/s]\n",
      "Generating train split: 100%|██████████| 550152/550152 [00:00<00:00, 4348367.57 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 550152/550152 [02:08<00:00, 4282.09 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 4334.01 examples/s]\n",
      "  0%|          | 56/275076 [01:26<117:22:40,  1.54s/it]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target -1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 40\u001b[0m\n\u001b[0;32m     26\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     27\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Replace with your output directory\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     29\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     30\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     35\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     36\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_encoded,\n\u001b[0;32m     37\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_encoded,\n\u001b[0;32m     38\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Evaluation function (replace with your metrics)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\transformers\\trainer.py:2163\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2161\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2164\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\transformers\\trainer.py:2523\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2516\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2517\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2520\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2521\u001b[0m )\n\u001b[0;32m   2522\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2523\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2526\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2529\u001b[0m ):\n\u001b[0;32m   2530\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2531\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\transformers\\trainer.py:3668\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3667\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3668\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3670\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3672\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3673\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3674\u001b[0m ):\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\transformers\\trainer.py:3722\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3720\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3721\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3722\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3723\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3724\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1700\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1699\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1700\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1702\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Course\\IBMGenAI\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target -1 is out of bounds."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SNLI dataset\n",
    "snli = load_dataset(\"stanfordnlp/snli\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "  premise = examples[\"premise\"]\n",
    "  hypothesis = examples[\"hypothesis\"]\n",
    "  return tokenizer(premise, hypothesis, padding=\"max_length\", truncation=True)\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Apply preprocessing to training and validation sets\n",
    "train_encoded = snli[\"train\"].map(preprocess_function, batched=True)\n",
    "val_encoded = snli[\"validation\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# Training function (replace with your training loop)\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Replace with your output directory\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_encoded,\n",
    "    eval_dataset=val_encoded,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluation function (replace with your metrics)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions, labels = trainer.predict(val_encoded)\n",
    "accuracy = accuracy_score(labels, predictions.argmax(-1))\n",
    "print(f\"Accuracy on validation set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98dee9-f05b-4a39-b360-32bd7c63c9f8",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5535aa-4727-4c24-9419-a20fb0a89dc7",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d873569-689f-45f0-ba3b-9db5cde1df86",
   "metadata": {},
   "source": [
    "[Fateme Akbari](https://author.skills.network/instructors/fateme_akbari)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ce750-2997-4286-ac83-5e12f6ebedb8",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "d29458bb8fd401d00186b91d1862005b641aee6fb66e97a6a065470e2b2b2981"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
